\documentclass[10pt]{article}

%%% Doc layout
\usepackage{fullpage} 
\usepackage{booktabs}       % professional-quality tables
\usepackage{microtype}      % microtypography
\usepackage{parskip}
\usepackage{times}

%% Hyperlinks always black, no weird boxes
\usepackage[hyphens]{url}
\usepackage[colorlinks=true,allcolors=black,pdfborder={0 0 0}]{hyperref}

%%% Math typesetting
\usepackage{amsmath,amssymb}

%%% Write out problem statements in blue, solutions in black
\usepackage{xcolor}
\newcommand{\officialdirections}[1]{{\color{purple} #1}}

%%% Avoid automatic section numbers (we'll provide our own)
\setcounter{secnumdepth}{0}

%% --------------
%% Header
%% --------------
\usepackage{fancyhdr}
\fancyhf{}
\fancyhead[C]{\ifnum\value{page}=1 CS 136 - 2023s - HW1 Submission \else \fi}
\fancyfoot[C]{\thepage} % page number
\renewcommand\headrulewidth{0pt}
\pagestyle{fancy}


%% --------------
%% Begin Document
%% --------------
\begin{document}

~~\\ %% add vertical space

{\Large{\bf Student Name: TODO}}

\Large{\bf Collaboration Statement:}

Total hours spent: TODO

I discussed ideas with these individuals:
\begin{itemize}
\item TODO
\item TODO
\item $\ldots$	
\end{itemize}

I consulted the following resources:
\begin{itemize}
\item TODO
\item TODO
\item $\ldots$	
\end{itemize}
~~\\
By submitting this assignment, I affirm this is my own original work that abides by the course collaboration policy.
~~\\
~~\\
Links: 
\href{https://www.cs.tufts.edu/cs/136/2023s/hw1.html}{[HW1 instructions]} 
\href{https://www.cs.tufts.edu/cs/136/2023s/index.html#collaboration}{[collab. policy]} 

\tableofcontents

\newpage

\officialdirections{
\subsection*{1a: Problem Statement}

Let $\rho \in (0.0, 1.0)$ be a Beta-distributed random variable: $p \sim \text{Beta}(a, b)$. 

Show that $\mathbb{E}[ \rho ] = \frac{a}{a + b}$.

**Hint:** You can use these identities, which hold for all $a > 0$ and $b > 0$:

\begin{align}
\Gamma(a) &= \int_{t=0}^{\infty} e^{-t} t^{a-1} dt
\\
\Gamma(a+1) &= a \Gamma(a)
\\
\int_{0}^1 \rho^{a-1} (1-\rho)^{b-1} d\rho &= \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}
\end{align}
}

\subsection{1a: Solution}
TODO YOUR SOLUTION HERE

\officialdirections{
\subsection*{1b: Problem Statement}

Let $\mu$ be a Dirichlet-distributed random variable: $\mu \sim \text{Dir}(a_1, \ldots a_V)$. 

Show that $\mathbb{E}[ \mu_w ] = \frac{a_w}{\sum_{v=1}^V a_v}$, for any integer $w$ that indexes a vocabulary word.

** Hint:** You can use the identity:
\begin{align}
\int \mu_1^{a_1-1} \mu_2^{a_2 - 1} \ldots \mu_V^{a_V-1} d\mu
 &= \frac
 	{\prod_{v=1}^V \Gamma(a_v)}
 	{\Gamma(a_1 + a_2 \ldots + a_V)}
\end{align}
}

\subsection{1b: Solution}
TODO YOUR SOLUTION HERE

\officialdirections{
\subsection*{2a: Problem Statement}

Show that the likelihood of all $N$ observed words can be written as:
\begin{align}
p(X_1 = x_1, X_2 = x_2, \ldots, X_N = x_N | \mu) = \prod_{v=1}^V \mu_v^{n_v}
\end{align}
}

\subsection{2a: Solution}
TODO YOUR SOLUTION HERE

\officialdirections{
\subsection*{2b: Problem Statement}

Derive the next-word posterior predictive, after integrating away parameter $\mu$.

That is, show that after seeing the $N$ training words, the probability of the next word $X_*$ being vocabulary word $v$ is:
\begin{align}
p( X_* = v | X_1 = x_1 \ldots X_N = x_n)
	&= \int p( X_* = v, \mu | X_1 = x_1 \ldots X_N = x_n) d\mu
\notag \\
	&= \frac{n_v + \alpha}{N + V\alpha}
\end{align}
}

\subsection{2b: Solution}

TODO YOUR SOLUTION HERE


\officialdirections{
\subsection*{2c: Problem Statement}
Derive the marginal likelihood of observed training data, after integrating away the parameter $\mu$.

That is, show that the marginal probability of the observed $N$ training words has the following closed-form expression:
\begin{align}
p( X_1 = x_1 \ldots X_N = x_N) 
	&= \int p( X_1 = x_1, \ldots X_N = x_N, \mu ) d\mu
	\\
	&= \frac
	{ \Gamma(V \alpha)      \prod_{v=1}^V \Gamma( n_v + \alpha ) }
	{ \Gamma(N + V \alpha ) \prod_{v=1}^V \Gamma(\alpha)         }
\end{align}
}


\subsection{2c: Solution}
TODO YOUR SOLUTION HERE


\end{document}
